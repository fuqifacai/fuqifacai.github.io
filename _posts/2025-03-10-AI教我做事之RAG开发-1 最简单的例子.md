---
layout:       post
title:        "AI教我做事之RAG开发-1 最简单的例子"
author:       "Ramendeus"
header-style: text
catalog:      true
tags:
    - AI Agent
    - RAG
    - Grok
    - AI教我做事
---

1.  概要

RAG，通过一种外挂信息的方式，补充LLM的能力。通常在每一个行业，都有专属的一些特殊名词或者解释，此时我们需要将这些文档，图像，数据等资料通过RAG的方式嵌入到LLM中，以便于我们在查询，聊天，或者AIAgent开发的时候用这些专属内容代替LLM的通用内容，使得最终的结果更加准确。

因为LLM具备幻觉和毒性，出来的结果内容并非确保一定的准确性，但是通过RAG，可以大幅解决这个问题。

不过，作者还是要提醒一句，RAG还是不能彻底解决大模型的毒性和幻觉问题。请谨慎使用和对待LLM的一切结果。

本文假定你对开发AI相关的内容已经有一定了解的开发人员。

同时要求你对Python编程语言有相对较高的认知和理解。因为很多技术人员喜欢Show me the codes.

所以我们解决问题的时候，直接用展示代码来说明问题，会相对较多。

更多资讯，请访问 [2img.ai](http://2img.ai/)

![AI教我做事之RAG开发-1 最简单的例子](https://www.shxcj.com/wp-content/uploads/2025/03/2025031002070849-706x1024.png)

2.  最简单的代码例子

```
from langchain_community.llms import Tongyi
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

#from langchain.document_loaders import TextLoader
from langchain_community.document_loaders import TextLoader

#from langchain.embeddings import OpenAIEmbeddings
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.indexes import VectorstoreIndexCreator
from langchain.text_splitter import CharacterTextSplitter

#from langchain.vectorstores import Chroma
from langchain_community.vectorstores import Chroma

#from langchain_community.llms import Ollama
from langchain_ollama import OllamaLLM
#from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

import os

# 创建文本加载器
## 这里需要确保该文本Utf8编码
loader = TextLoader('D:\\RAG-TestData-1.txt', encoding='utf8')
print("创建文本加载器")

# 加载文档
documents = loader.load()
print("加载文档")

# 文本分块
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
splits = text_splitter.split_documents(documents)
print("文本分块")

# 创建嵌入模型
myEmbedding = HuggingFaceEmbeddings()
print("创建嵌入模型")

# 创建向量库
db = Chroma.from_documents(splits, myEmbedding)
print("创建向量库")

# 将向量库转换为检索器
retriever = db.as_retriever()
print("将向量库转换为检索器")

# 准备大模型
# # 这些Key请在实际使用中替换成你自己的,这里使用通义千问
# os.environ["DASHSCOPE_API_KEY"] = "sk-5b3bdd33fe584683a83b3a032366cf9a"
# DASHSCOPE_API_KEY="sk-5b3bdd33fe584683a83b3a032366cf9a"
# myLLM=Tongyi(temperature=1)
myLLM = OllamaLLM(model="llama2")
print("准备大模型")

# 创建检索问答系统
qa_chain = RetrievalQA.from_chain_type(llm=myLLM, chain_type="stuff", retriever=retriever)
print("创建检索问答系统")

result=qa_chain.invoke('字形绘梦这个产品是哪个公司出的')

print(f"回答: {result['result'].strip()}")
```

主要的流程是：

1.  创建LLM
2.  创建Document，读取本地文档
3.  进行文本分块
4.  创建嵌入模型
5.  将嵌入模型和分块后的文本，创建出DB（向量存储对象中），向量化处理
6.  创建检索问答系统.
7.  创建Prompt+LLM后创建LangChain
8.  向chain提问

3.  RAG开发关键问题集锦

![AI教我做事之RAG开发-1 最简单的例子](https://www.shxcj.com/wp-content/uploads/2025/03/7f73e65af918dba832f52372e61294f-910x1024.jpg)

## 3.1 如何使用大模型，进行调用

### 3.1.1以下是调用通义千问

通义千问是阿里的在线大模型，使用的时候需要API\_KEY，去它的官网注册获取即可

```
from langchain_community.llms import Tongyi
os.environ["DASHSCOPE_API_KEY"] = "sk-XXX"
DASHSCOPE_API_KEY="sk-XXX"
# 这个例子中我们使用通义千问的LLM来处理，依赖引入from langchain_community.llms import Tongyi
myLLM=Tongyi(temperature=1)
```

### 3.1.2 以下是调用本地Ollama

Ollama这里我们使用本地大模型，不需要什么API\_KEY，直接调用。

不过在某个地方需要指定APIService的地址即可。比如默认的http://127.0.0.1:11434

```
from langchain_community.llms import Ollama
llm = Ollama(model="llama3.3")
```

### 3.1.3 如何使用Huggingface的LLM模型？

方法一：

```
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
model_id = "THUDM/chatglm3-6b"
tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, device_map="auto")
pipe = pipeline(
    "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=2000
)
llm = HuggingFacePipeline(pipeline=pipe)
```

方法二：

```
from langchain.llms import HuggingFacePipeline
model_id = "THUDM/chatglm3-6b"
llm = HuggingFacePipeline.from_model_id(
    model_id=model_id,
    task="text-generation",
    device=0,
    model_kwargs={"temperature": 0.01, "max_length": 2000, "trust_remote_code": True},
)
```

## 3.2 如何修改为读取本地文档？

### 3.2.1 读取MD,from langchain\_community.document\_loaders import PyPDFLoader文本文件

使用TextLoader即可，

```
from langchain_community.document_loaders import TextLoader
loader = TextLoader("./index.md")
loader = TextLoader('D:\\RAG-TestData-1.txt', encoding='utf8')
docs = loader.load()
```

### 3.2.2 读取Html文件

```
from langchain_community.document_loaders import UnstructuredHTMLLoader
loader = UnstructuredHTMLLoader("example_data/fake-content.html")
docs = loader.load()
```

### 3.2.3 读取PDF文件

```
from langchain_community.document_loaders import PyPDFLoader
loader = PyPDFLoader("D:\\RAG-TestImage-3.pdf")
docs = loader.load()
```

## 3.3 文本分块

chunk\_size,一般的建议是小文档500,大文档1000以上

chunk\_overlap，设置50

写法1

```
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
splits = text_splitter.split_documents(documents)
```

写法2

```
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
```

## 3.4 创建嵌入模型

### 3.4.1 如何通过Ollama使用Embedding模型？

Ollama可以支持Embedding模型。 [https://ollama.com/blog/embedding-models](https://ollama.com/blog/embedding-models)

```
# 在CMD中执行命令，下载模型 ollama pull mofanke/acge_text_embedding
# 运行模型
from langchain_community.embeddings import OllamaEmbeddings
model_name = "mofanke/acge_text_embedding"
vectorstore = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(model=model_name))
```

但是不支持Rerank模型，你可以使用Huggingface Transformers开发库来实现。

### 3.4.2 如何使用Huggingface的Embedding模型

```
model_name = "maidalun1020/bce-embedding-base_v1"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}
myEmbedding = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
vectorstore = Chroma.from_documents(documents=splits, embedding=myEmbedding )
```

或最简单的一句话 myEmbedding = HuggingFaceBgeEmbeddings()

### 3.5 创建向量库和索引器

### 3.5.1 创建向量库

db = Chroma.from\_documents(documents=splits,embedding=myEmbedding )

### 3.5.2 将向量库转换为检索器

retriever = db.as\_retriever()

### 如何修改RAG检索返回的文档数量？

只返回top K 个检索结果

```
retriever = db.as_retriever(search_kwargs={"k": 1})
```

## 3.6 创建关键字+LLM，创建Chain

### 3.6.1 如何将prompt提示词中的文本修改为中文？

直接在提示的模板中，要求用中文回答即可。

### 3.6.2 如何使用自定义prompt提示词？

```
# 4. 创建RAG链
def create_rag_chain(vector_store, llm):
# 自定义提示模板，只要求简洁回答
    prompt_template = """根据以下上下文，直接用中文回答问题，不要添加多余内容。
上下文: {context}
问题: {question}
回答: """
    
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    
    # 创建RetrievalQA链
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=False,  # 不返回来源文档
        chain_type_kwargs={"prompt": PROMPT}  # 使用自定义提示
    )
    return qa_chain

```
