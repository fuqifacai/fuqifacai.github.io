---
layout:       post
title:        "第四十九章 添加滤镜能力"
author:       "Ramendeus"
header-style: text
catalog:      true
tags:
    - 流式
    - WebRTC
    - Streaming
---

[访问 2img.ai 官网以获取更多AI/AIGC信息](https://2img.ai)
本文大部分配图使用微信小程序【字形绘梦】免费生成。
fuqifai.github.io

## 第四十九章 添加滤镜能力


## 添加滤镜后端做法

由于需要实时预览滤镜效果，所以必须在渲染开始之前添加滤镜。好在 WebRTC 已经提供了 VideoProcessor 这个接口类，可以对采集到的帧数据进行预处理，调用 `VideoSource.setVideoProcessor(processor)` 即可设置：

VideoSource.java

```
public class VideoSource extends MediaSource {
  // other definitions...

  private final CapturerObserver capturerObserver = new CapturerObserver() {
    // other definitions...

    @Override
    public void onFrameCaptured(VideoFrame frame) {
      final VideoProcessor.FrameAdaptationParameters parameters =
          nativeAndroidVideoTrackSource.adaptFrame(frame);
      synchronized (videoProcessorLock) {
        if (videoProcessor != null) {

          videoProcessor.onFrameCaptured(frame, parameters);
          return;
        }
      }

      // body method...
    }
  };

  // other definitions...
}
```

最后给出一个 VideoProcessor 的简单实现，给各位读者参考：

```
public final class Example implements VideoProcessor {

  @Nullable private VideoSink mVideoSink;

  @Override
  public void onCapturerStarted(boolean success) {
    // DO SOMETHING IF YOU WANT.
  }

  @Override
  public void onCapturerStopped() {
    // DO SOMETHING IF YOU WANT.
  }

  @Override
  public void setSink(@Nullable VideoSink sink) {
    // 需要持有 WebRTC 传入的 VideoSink 对象
    mVideoSink = sink;
  }

  @Override
  public void onFrameCaptured(@NonNull VideoFrame frame) {
    VideoFrame newFrame = yourVideoFilter(frame);

    // 会调用 NativeAndroidVideoTrackSource 将新的帧数据传递给 Native 层
    if (mVideoSink != null) mVideoSink.onFrame(frame);
  }
}
```

## 对应的前端可能有不同的做法

涉及到对视频流的实时处理。你可以使用WebRTC的`MediaStream`和`Canvas` API来实现视频滤镜。以下是一个简单的示例，演示了如何为WebRTC视频流应用一个基本的灰度滤镜。

### 实现步骤

1.  **获取视频流**：使用`getUserMedia`从摄像头获取视频流。
2.  **将视频流绘制到Canvas**：通过Canvas API对视频帧进行处理。
3.  **将处理后的视频流显示到****`<video>`****元素**。

### 示例代码

以下是一个简单的JavaScript示例，展示了如何应用灰度滤镜到WebRTC视频流：

```
<!DOCTYPE html>
<html>
<head>
    <title>WebRTC Video Filter</title>
    <style>
        video, canvas {
            display: block;
            margin: 10px auto;
        }
    </style>
</head>
<body>
    <video id="video" autoplay></video>
    <canvas id="canvas"></canvas>

    <script>
        async function startVideo() {
            const video = document.getElementById('video');
            const canvas = document.getElementById('canvas');
            const ctx = canvas.getContext('2d');

            // Get user media
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;

            // Set canvas dimensions
            video.onloadedmetadata = () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                drawFrame();
            };

            function drawFrame() {
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                
                // Apply grayscale filter
                const frame = ctx.getImageData(0, 0, canvas.width, canvas.height);
                const data = frame.data;
                for (let i = 0; i < data.length; i += 4) {
                    const avg = (data[i] + data[i + 1] + data[i + 2]) / 3;
                    data[i] = avg;     // Red
                    data[i + 1] = avg; // Green
                    data[i + 2] = avg; // Blue
                }
                ctx.putImageData(frame, 0, 0);

                // Request the next frame
                requestAnimationFrame(drawFrame);
            }
        }

        startVideo();
    </script>
</body>
</html>
```

### 代码解释

1.  **获取视频流**：通过`navigator.mediaDevices.getUserMedia`获取视频流并将其设置到`<video>`元素。
2.  **Canvas绘制**：使用Canvas API将视频帧绘制到Canvas上。
3.  **滤镜应用**：在`drawFrame`函数中获取Canvas的图像数据，对其进行灰度化处理，并将结果绘制回Canvas。
4.  **动画循环**：使用`requestAnimationFrame`实现每帧更新，以确保实时处理视频流。

你可以在这个基础上，使用不同的图像处理算法和滤镜效果来扩展功能。

