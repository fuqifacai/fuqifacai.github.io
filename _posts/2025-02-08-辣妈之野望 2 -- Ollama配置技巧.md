---
layout:       post
title:        "辣妈之野望 2 –Ollama配置技巧"
author:       "Ramendeus"
header-style: text
catalog:      true
tags:
    - Ollama
    - LLM
    - WebUI Lite
    - GPT
    - 个人大模型
---

## 辣妈之野望 2 –Ollama配置技巧


在0.1.13 之后的版本Ollama**支持用户访问多并发和模型加载多并发。**

这可以让我们不需要排队。当然如果在同一台机器上运行，虽然不排队，但是并行计算所要求的GPU处理能力和显存要求还是一个总和。分布式就没这个问题。

当然个人用户可以简单用用，也是不错。

更多技术资讯下载: [2img.ai](https://link.juejin.cn/?target=https%3A%2F%2F2img.ai)

相关配图由微信小程序【字形绘梦】免费生成

![辣妈之野望 2 --Ollama配置技巧](https://www.shxcj.com/wp-content/uploads/2025/02/70fdf6d6-56f6-42d2-9391-f538c077a037.jpg)

### 第一步，确保版本支持。

在Cmd中执行ollama -v 查看当前版本。如下图

![辣妈之野望 2 --Ollama配置技巧](https://www.shxcj.com/wp-content/uploads/2025/02/012fa609-d872-4ff3-a930-3656e26e2ea2.png)

### 第二步，如何控制用户访问多并发和模型加载多并发呢？

直接在系统的环境变量中增加几个设置。这里以windows为例子，linux的请对应修改。

OLLAMA\_NUM\_PARALLEL=8 设置8个用户并发请求

OLLAMA\_MAX\_LOADED\_MODELS=8 设置同时加载8个模型

类似下图中

![辣妈之野望 2 --Ollama配置技巧](https://www.shxcj.com/wp-content/uploads/2025/02/590810fb-3572-48d8-83a0-5022a5d2bb90.png)

### 其余重要的参数设置，也可以同样的设置。

### 罗列如下

OLLAMA\_HOST=0.0.0.0 允许外网访问

OLLAMA\_MODELS=E:\\MyOllamaFolders 解决模型默认下载C 盘带来磁盘空间不够的问题

OLLAMA\_KEEP\_ALIVE=24h 设置模型加载到内存中保持24个小时(默认情况下，模型在卸载之前会在内存中保留 5 分钟)

OLLAMA\_HOST=0.0.0.0:9871 修改默认端口11434端口到这里的9871端口

OLLAMA\_NUM\_PARALLEL=8 设置8个用户并发请求

OLLAMA\_MAX\_LOADED\_MODELS=8 设置同时加载8个模型

![辣妈之野望 2 --Ollama配置技巧](https://www.shxcj.com/wp-content/uploads/2025/02/7d1b8c5f-5bf6-478e-9a98-9d9a7f403914.jpg)

